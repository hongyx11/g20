{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset and save it in the same dir with this file.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "kf = KFold(n_splits=10)\n",
    "# import scikit learn\n",
    "from sklearn import linear_model,svm\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "folder = ['datasets/' + chr(ord('A') + x) for x in range(18)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getxandy(filename):\n",
    "  with open(filename,'r') as f:\n",
    "    df = pd.read_csv(f,index_col=0)\n",
    "    x = np.zeros((len(df.x),2))\n",
    "    y = np.zeros((len(df.x)))\n",
    "    x[:,0] = df.x.to_numpy()\n",
    "    x[:,1] = df.y.to_numpy()\n",
    "    y = df.z.to_numpy()\n",
    "  return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getpolyxandy(filename,poly=2):\n",
    "  with open(filename,'r') as f:\n",
    "    df = pd.read_csv(f,index_col=0)\n",
    "    x = np.zeros((len(df.x),2))\n",
    "    y = np.zeros((len(df.x)))\n",
    "    x[:,0] = df.x.to_numpy()\n",
    "    x[:,1] = df.y.to_numpy()\n",
    "    y = df.z.to_numpy()\n",
    "    poly = PolynomialFeatures(degree=poly)\n",
    "    x = poly.fit_transform(x)\n",
    "  return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreperdataset(y_pred,y_test):\n",
    "  return np.sqrt(np.sum(np.square(y_pred - y_test))/len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fit once\n",
      "loss for f is  0.13077150399420454\n",
      "fit once\n",
      "loss for f is  0.16722950523914298\n",
      "fit once\n",
      "loss for f is  0.4189126118939837\n",
      "fit once\n",
      "loss for f is  0.037751152219778684\n",
      "fit once\n",
      "loss for f is  0.052454433405627096\n",
      "fit once\n",
      "loss for f is  0.22827530533597518\n",
      "fit once\n",
      "loss for f is  0.019048924163380953\n",
      "fit once\n",
      "loss for f is  0.12267405511406013\n",
      "fit once\n",
      "loss for f is  0.22085679252635285\n",
      "final average is  0.15533047598805622\n"
     ]
    }
   ],
   "source": [
    "# pre 9\n",
    "model = Pipeline([('poly', PolynomialFeatures(degree=4)),('linear', DecisionTreeRegressor(max_depth=25))])\n",
    "finalacc = []\n",
    "for f in folder[:9]:\n",
    "  X,y = getxandy(f+'/train.csv')\n",
    "  reg = model\n",
    "  for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    y_max = y_train.max()\n",
    "    y_min = y_train.min()\n",
    "    y_train_scaled = (y_train - y_min) / (y_max - y_min)\n",
    "    reg.fit(X_train,y_train_scaled)\n",
    "    print(\"fit once\")\n",
    "    break\n",
    "  y_pred_scaled = reg.predict(X_test)\n",
    "  y_pred = y_pred_scaled * (y_max - y_min) + y_min\n",
    "  loss = scoreperdataset(y_pred, y_test) \n",
    "  finalacc.append(loss)\n",
    "  print('loss for f is ', loss)\n",
    "print('final average is ', sum(finalacc)/len(finalacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post 9\n",
    "model = Pipeline([('poly', PolynomialFeatures(degree=4)),('linear', GaussianProcessRegressor())])\n",
    "finalacc = []\n",
    "for f in folder[9:]:\n",
    "  X,y = getxandy(f+'/train.csv')\n",
    "  reg = model\n",
    "  for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    y_max = y_train.max()\n",
    "    y_min = y_train.min()\n",
    "    y_train_scaled = (y_train - y_min) / (y_max - y_min)\n",
    "    reg.fit(X_train,y_train_scaled)\n",
    "    print(\"fit once\")\n",
    "    break\n",
    "  y_pred_scaled = reg.predict(X_test)\n",
    "  y_pred = y_pred_scaled * (y_max - y_min) + y_min\n",
    "  loss = scoreperdataset(y_pred, y_test) \n",
    "  finalacc.append(loss)\n",
    "  print('loss for f is ', loss)\n",
    "print('final average is ', sum(finalacc)/len(finalacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=DecisionTreeRegressor(),\n",
       "             param_grid={'max_depth': [10, 15, 20, 25, 30]})"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "f = 'datasets/R'\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "X,y = getpolyxandy(f+'/train.csv',4)\n",
    "clf = GridSearchCV(DecisionTreeRegressor(), {'max_depth':[10,15,20,25,30]})\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=1)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss for f is  0.655048022520679\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.best_estimator_.predict(X_test)\n",
    "print('loss for f is ', scoreperdataset(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f='datasets/R'\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "X,y = getxandy(f+'/train.csv')\n",
    "#X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx = sorted(X,key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.000107 0.242100\n0.000122 0.621195\n0.000137 0.385122\n0.000153 0.208179\n0.000183 0.787762\n0.000183 0.400732\n0.000183 0.846815\n0.000198 0.200565\n0.000214 0.446220\n0.000214 0.052811\n0.000214 0.749203\n0.000229 0.012421\n0.000229 0.781109\n0.000244 0.157641\n0.000244 0.072297\n0.000259 0.447715\n0.000259 0.827756\n0.000275 0.564187\n0.000275 0.854383\n0.000290 0.505623\n0.000290 0.043809\n0.000290 0.592432\n0.000305 0.821439\n0.000320 0.794354\n0.000336 0.485252\n0.000351 0.450187\n0.000366 0.293523\n0.000366 0.173175\n0.000366 0.116777\n0.000381 0.090547\n0.000381 0.044648\n0.000381 0.212802\n0.000397 0.572595\n0.000412 0.474311\n0.000412 0.473396\n0.000427 0.890684\n0.000427 0.269596\n0.000443 0.516182\n0.000458 0.949355\n0.000458 0.695613\n0.000473 0.500618\n0.000473 0.333623\n0.000488 0.745373\n0.000504 0.714611\n0.000504 0.934371\n0.000519 0.020722\n0.000534 0.935836\n0.000549 0.302541\n0.000565 0.901717\n0.000565 0.282750\n0.000580 0.531762\n0.000595 0.206577\n0.000610 0.106279\n0.000610 0.319875\n0.000610 0.017517\n0.000610 0.550439\n0.000626 0.074876\n0.000626 0.488655\n0.000626 0.643793\n0.000641 0.246799\n0.000671 0.546380\n0.000671 0.441611\n0.000671 0.122240\n0.000687 0.677134\n0.000687 0.128359\n0.000687 0.750698\n0.000717 0.965469\n0.000732 0.720790\n0.000732 0.603418\n0.000748 0.277409\n0.000748 0.472694\n0.000748 0.554330\n0.000748 0.341695\n0.000763 0.098833\n0.000793 0.459281\n0.000793 0.713741\n0.000809 0.495674\n0.000809 0.083207\n0.000809 0.477638\n0.000839 0.862181\n0.000839 0.796826\n0.000839 0.130648\n0.000839 0.037675\n0.000855 0.295476\n0.000855 0.261357\n0.000870 0.817365\n0.000870 0.986206\n0.001114 0.231159\n0.001144 0.533623\n0.001144 0.060121\n0.001160 0.713451\n0.001175 0.648386\n0.001190 0.127840\n0.001205 0.190234\n0.001205 0.423178\n0.001236 0.043732\n0.001251 0.506325\n0.001266 0.124498\n0.001266 0.495827\n0.001266 0.009812\n0.001282 0.467353\n0.001282 0.254353\n0.001282 0.194629\n0.001297 0.888228\n0.001297 0.855894\n0.001297 0.877470\n0.001312 0.294728\n0.001312 0.741176\n0.001328 0.635462\n0.001343 0.346487\n0.001343 0.139727\n0.001358 0.510658\n0.001358 0.702464\n0.001358 0.168185\n0.001373 0.849836\n0.001373 0.811337\n0.001373 0.361639\n0.001389 0.349172\n0.001389 0.354147\n0.001419 0.879789\n0.001450 0.673533\n0.001450 0.462806\n0.001465 0.380514\n0.001465 0.388144\n0.001465 0.020676\n0.001480 0.903731\n0.001480 0.870451\n0.001480 0.514427\n0.001480 0.377340\n0.001495 0.624323\n0.001511 0.037537\n0.001526 0.187243\n0.001526 0.965713\n0.001526 0.644388\n0.001526 0.385153\n0.001541 0.253361\n0.001541 0.686687\n0.001541 0.166827\n0.001556 0.430182\n0.001572 0.005722\n0.001587 0.424506\n0.001602 0.140261\n0.001602 0.816587\n0.001602 0.610285\n0.001602 0.149783\n0.001617 0.917144\n0.001617 0.857740\n0.001633 0.863859\n0.001633 0.287419\n0.001633 0.303593\n0.001678 0.212848\n0.001694 0.790188\n0.001694 0.766827\n0.001694 0.473121\n0.001694 0.752239\n0.001694 0.850111\n0.001709 0.840543\n0.001709 0.448661\n0.001724 0.536706\n0.001740 0.569528\n0.001740 0.746715\n0.001755 0.229435\n0.001755 0.842420\n0.001770 0.831418\n0.001770 0.332509\n0.001816 0.767498\n0.001831 0.146441\n0.001831 0.356878\n0.001846 0.642451\n0.001846 0.225681\n0.001877 0.851484\n0.002106 0.566583\n0.002106 0.496300\n0.002136 0.724727\n0.002152 0.446754\n0.002152 0.371542\n0.002152 0.730251\n0.002152 0.785382\n0.002167 0.310323\n0.002167 0.346807\n0.002197 0.250278\n0.002197 0.829633\n0.002197 0.722255\n0.002213 0.969177\n0.002213 0.576837\n0.002213 0.988128\n0.002213 0.716594\n0.002228 0.133410\n0.002228 0.491371\n0.002228 0.174594\n0.002228 0.225406\n0.002228 0.911742\n0.002228 0.585748\n0.002243 0.124590\n0.002243 0.915373\n0.002243 0.072221\n0.002258 0.793423\n0.002258 0.063447\n0.002274 0.731273\n0.002274 0.147601\n0.002274 0.002335\n0.002304 0.139162\n0.002304 0.057313\n0.002304 0.681788\n0.002319 0.764355\n0.002335 0.399466\n0.002335 0.787442\n0.002335 0.194720\n0.002350 0.158755\n0.002350 0.455100\n0.002350 0.946639\n0.002350 0.051575\n0.002396 0.046754\n0.002396 0.511620\n0.002411 0.363531\n0.002411 0.773693\n0.002426 0.533455\n0.002441 0.634119\n0.002441 0.524834\n0.002441 0.095689\n0.002441 0.763500\n0.002457 0.705287\n0.002457 0.994339\n0.002457 0.259190\n0.002472 0.812161\n0.002472 0.573495\n0.002487 0.987182\n0.002487 0.186816\n0.002518 0.456779\n0.002518 0.832151\n0.002518 0.515496\n0.002533 0.859434\n0.002533 0.081453\n0.002548 0.036637\n0.002548 0.088228\n0.002564 0.825162\n0.002579 0.278691\n0.002594 0.943359\n0.002594 0.909163\n0.002609 0.351644\n0.002625 0.367544\n0.002625 0.219364\n0.002640 0.912490\n0.002640 0.827527\n0.002655 0.257649\n0.002655 0.675273\n0.002655 0.814160\n0.002655 0.400320\n0.002670 0.199405\n0.002686 0.673869\n0.002701 0.494713\n0.002701 0.574594\n0.002701 0.509880\n0.002716 0.376516\n0.002731 0.893187\n0.002747 0.422232\n0.002747 0.723171\n0.002747 0.521767\n0.002747 0.810880\n0.002762 0.770886\n0.002777 0.941726\n0.002777 0.669306\n0.002792 0.777859\n0.002823 0.440589\n0.002838 0.539620\n0.002853 0.592615\n0.002853 0.695583\n0.002853 0.420493\n0.002869 0.595560\n0.002869 0.552209\n0.002869 0.211673\n0.002899 0.853620\n0.003113 0.811795\n0.003113 0.506233\n0.003113 0.522606\n0.003113 0.217517\n0.003128 0.023758\n0.003143 0.185534\n0.003143 0.317754\n0.003174 0.180896\n0.003174 0.193835\n0.003189 0.085359\n0.003189 0.732845\n0.003204 0.914137\n0.003204 0.226734\n0.003220 0.466423\n0.003220 0.882780\n0.003220 0.077760\n0.003220 0.236484\n0.003235 0.276646\n0.003250 0.974853\n0.003265 0.338231\n0.003281 0.030503\n0.003281 0.496620\n0.003296 0.282551\n0.003311 0.863142\n0.003326 0.309346\n0.003326 0.972244\n0.003326 0.971130\n0.003342 0.949340\n0.003357 0.211566\n0.003372 0.130541\n0.003388 0.656672\n0.003403 0.595743\n0.003403 0.439307\n0.003403 0.399191\n0.003433 0.554131\n0.003433 0.916564\n0.003449 0.899718\n0.003464 0.248417\n0.003494 0.374136\n0.003494 0.612436\n0.003510 0.723339\n0.003510 0.055360\n0.003510 0.658473\n0.003510 0.488441\n0.003510 0.633341\n0.003525 0.990860\n0.003555 0.698634\n0.003571 0.175219\n0.003571 0.347860\n0.003586 0.285603\n0.003601 0.218830\n0.003632 0.777508\n0.003632 0.820142\n0.003632 0.795270\n0.003632 0.504173\n0.003647 0.108568\n0.003647 0.646418\n0.003647 0.800198\n0.003662 0.135607\n0.003662 0.472618\n0.003662 0.179385\n0.003677 0.667247\n0.003677 0.206317\n0.003708 0.200214\n0.003723 0.512779\n0.003723 0.617884\n0.003723 0.387259\n0.003738 0.263890\n0.003754 0.358251\n0.003754 0.647562\n0.003754 0.158312\n0.003769 0.941451\n0.003769 0.956649\n0.003784 0.202289\n0.003799 0.408621\n0.003799 0.490852\n0.003815 0.361212\n0.003830 0.346563\n0.003830 0.853758\n0.003845 0.947646\n0.003845 0.245335\n0.003845 0.736370\n0.003861 0.782452\n0.003861 0.102235\n0.003861 0.058381\n0.003861 0.866606\n0.003876 0.614588\n0.003891 0.575509\n0.004105 0.598642\n0.004105 0.974426\n0.004105 0.224384\n0.004120 0.543099\n0.004135 0.301198\n0.004150 0.685710\n0.004150 0.635782\n0.004150 0.724147\n0.004150 0.836881\n0.004150 0.485695\n0.004150 0.984680\n0.004166 0.263844\n0.004181 0.055360\n0.004181 0.962387\n0.004196 0.230426\n0.004211 0.177661\n0.004211 0.898848\n0.004242 0.843763\n0.004242 0.710460\n0.004257 0.770443\n0.004257 0.915221\n0.004257 0.546426\n0.004257 0.062181\n0.004273 0.486641\n0.004273 0.117449\n0.004288 0.014130\n0.004303 0.617258\n0.004318 0.834470\n0.004318 0.554208\n0.004334 0.303639\n0.004364 0.863111\n0.004364 0.783841\n0.004364 0.098756\n0.004364 0.198398\n0.004379 0.569848\n0.004379 0.979217\n0.004395 0.680400\n0.004410 0.338262\n0.004425 0.536172\n0.004425 0.065873\n0.004440 0.412604\n0.004440 0.175601\n0.004456 0.125689\n0.004471 0.525154\n0.004486 0.064790\n0.004486 0.377401\n0.004501 0.142229\n0.004501 0.560250\n0.004501 0.763760\n0.004501 0.043641\n0.004517 0.109468\n0.004517 0.548882\n0.004532 0.218219\n0.004532 0.358450\n0.004532 0.693263\n0.004547 0.501366\n0.004547 0.360525\n0.004547 0.623590\n0.004578 0.407767\n0.004593 0.887343\n0.004593 0.387701\n0.004593 0.819104\n0.004623 0.160845\n0.004639 0.989456\n0.004639 0.520333\n0.004639 0.965545\n0.004669 0.183490\n0.004669 0.528725\n0.004669 0.811658\n0.004685 0.447486\n0.004685 0.980117\n0.004685 0.209110\n0.004685 0.959609\n0.004685 0.661769\n0.004685 0.558145\n0.004700 0.612818\n0.004715 0.919600\n0.004715 0.029374\n0.004715 0.051286\n0.004715 0.172808\n0.004730 0.001328\n0.004730 0.975280\n0.004730 0.507317\n0.004746 0.858793\n0.004761 0.058213\n0.004761 0.600732\n0.004761 0.998352\n0.004776 0.607889\n0.004776 0.760128\n0.004791 0.187304\n0.004837 0.700511\n0.004852 0.957504\n0.004852 0.938613\n0.004852 0.849134\n0.005112 0.122301\n0.005112 0.202274\n0.005112 0.797772\n0.005112 0.440681\n0.005142 0.836606\n0.005142 0.875502\n0.005142 0.647501\n0.005173 0.421393\n0.005173 0.278218\n0.005173 0.827863\n0.005188 0.297627\n0.005203 0.718486\n0.005219 0.865415\n0.005234 0.448280\n0.005234 0.486366\n0.005249 0.632761\n0.005280 0.813504\n0.005295 0.974899\n0.005295 0.722393\n0.005310 0.361395\n0.005310 0.452140\n0.005325 0.481422\n0.005325 0.329458\n0.005325 0.978851\n0.005341 0.644160\n0.005356 0.090593\n0.005371 0.055726\n0.005371 0.900771\n0.005371 0.662669\n0.005386 0.973251\n0.005386 0.855283\n0.005402 0.247547\n0.005402 0.633448\n0.005417 0.237858\n0.005432 0.478462\n0.005447 0.712200\n0.005463 0.551644\n0.005463 0.186374\n0.005478 0.051667\n0.005478 0.521263\n0.005478 0.809659\n0.005478 0.856672\n0.005493 0.720516\n0.005493 0.830579\n0.005493 0.204517\n0.005509 0.636606\n0.005524 0.463569\n0.005524 0.080888\n0.005539 0.280720\n0.005539 0.028473\n0.005539 0.224811\n0.005554 0.507698\n0.005554 0.665797\n0.005570 0.344793\n0.005585 0.579431\n0.005600 0.832837\n0.005600 0.266896\n0.005600 0.214664\n0.005661 0.706676\n0.005661 0.812406\n0.005676 0.585168\n0.005692 0.231266\n0.005692 0.160540\n0.005722 0.506645\n0.005737 0.744366\n0.005737 0.285466\n0.005737 0.701747\n0.005737 0.067292\n0.005737 0.083101\n0.005737 0.629633\n0.005753 0.758495\n0.005753 0.917678\n0.005753 0.427298\n0.005768 0.029450\n0.005783 0.027527\n0.005798 0.831754\n0.005798 0.814252\n0.005814 0.255314\n0.005814 0.201724\n0.005829 0.313741\n0.005844 0.527459\n0.005844 0.684611\n0.005859 0.635615\n0.005875 0.962127\n0.005890 0.574746\n0.005890 0.148806\n0.006104 0.919234\n0.006119 0.679133\n0.006119 0.737438\n0.006134 0.426581\n0.006134 0.581491\n0.006149 0.535760\n0.006165 0.700374\n0.006165 0.397330\n0.006180 0.398123\n0.006195 0.444099\n0.006195 0.440803\n0.006195 0.291325\n0.006195 0.726513\n0.006210 0.580591\n0.006226 0.625681\n0.006241 0.558145\n0.006271 0.412451\n0.006287 0.621897\n0.006287 0.564614\n0.006287 0.349844\n0.006287 0.806714\n0.006287 0.973663\n0.006302 0.069520\n0.006302 0.395743\n0.006317 0.530800\n0.006317 0.551461\n0.006317 0.199802\n0.006317 0.344137\n0.006332 0.493263\n0.006332 0.509545\n0.006348 0.262669\n0.006363 0.157122\n0.006363 0.005478\n0.006363 0.821729\n0.006378 0.370642\n0.006378 0.411109\n0.006378 0.589776\n0.006394 0.273182\n0.006409 0.325200\n0.006424 0.705806\n0.006424 0.372732\n0.006439 0.366690\n0.006455 0.234516\n0.006455 0.825406\n0.006485 0.447196\n0.006485 0.221225\n0.006485 0.799512\n0.006500 0.787518\n0.006500 0.590158\n0.006531 0.990600\n0.006531 0.026307\n0.006546 0.090471\n0.006561 0.039734\n0.006577 0.708858\n0.006577 0.423651\n0.006577 0.533104\n0.006577 0.124193\n0.006622 0.817121\n0.006622 0.446677\n0.006653 0.460380\n0.006653 0.017365\n0.006653 0.054444\n0.006653 0.003479\n0.006668 0.970153\n0.006668 0.899229\n0.006668 0.657557\n0.006683 0.823392\n0.006683 0.769192\n0.006683 0.241520\n0.006699 0.615778\n0.006699 0.302571\n0.006699 0.401282\n0.006714 0.753658\n0.006714 0.541405\n0.006714 0.064149\n0.006714 0.275471\n0.006714 0.560510\n0.006729 0.025559\n0.006729 0.470802\n0.006729 0.798672\n0.006729 0.028656\n0.006729 0.400519\n0.006744 0.019608\n0.006744 0.874403\n0.006760 0.521904\n0.006760 0.764782\n0.006760 0.837797\n0.006760 0.636561\n0.006775 0.266850\n0.006775 0.455177\n0.006790 0.053803\n0.006790 0.002365\n0.006790 0.438453\n0.006821 0.965682\n0.006821 0.058839\n0.006836 0.308675\n0.006836 0.836866\n0.006851 0.771832\n0.006851 0.940688\n0.006851 0.822782\n0.006851 0.623407\n0.006882 0.936614\n0.006882 0.878553\n0.006882 0.131609\n0.007111 0.708125\n0.007126 0.338674\n0.007126 0.328542\n0.007126 0.276432\n0.007141 0.315465\n0.007141 0.959670\n0.007156 0.721752\n0.007172 0.398489\n0.007172 0.435233\n0.007172 0.990753\n0.007187 0.610605\n0.007187 0.701595\n0.007187 0.963775\n0.007202 0.801556\n0.007233 0.762554\n0.007263 0.690715\n0.007263 0.523781\n0.007263 0.271809\n0.007279 0.628443\n0.007279 0.938766\n0.007294 0.313542\n0.007294 0.986618\n0.007294 0.193759\n0.007309 0.359518\n0.007324 0.813519\n0.007324 0.678523\n0.007340 0.145434\n0.007340 0.922301\n0.007385 0.230167\n0.007385 0.549828\n0.007401 0.519692\n0.007416 0.974243\n0.007416 0.461311\n0.007431 0.603738\n0.007462 0.147768\n0.007462 0.654795\n0.007477 0.478676\n0.007492 0.645457\n0.007492 0.925216\n0.007492 0.364736\n0.007492 0.739864\n0.007507 0.703792\n0.007507 0.219425\n0.007507 0.098589\n0.007507 0.376516\n0.007523 0.615244\n0.007523 0.304555\n0.007523 0.013886\n0.007538 0.698161\n0.007538 0.063691\n0.007538 0.025711\n0.007553 0.966705\n0.007553 0.264637\n0.007553 0.975586\n0.007553 0.980697\n0.007553 0.518563\n0.007568 0.350668\n0.007568 0.390127\n0.007568 0.600488\n0.007584 0.949325\n0.007584 0.141863\n0.007599 0.059510\n0.007645 0.714275\n0.007645 0.003738\n0.007675 0.432273\n0.007691 0.254780\n0.007691 0.340475\n0.007691 0.134768\n0.007706 0.818555\n0.007736 0.291814\n0.007736 0.086488\n0.007752 0.863737\n0.007752 0.579355\n0.007767 0.210575\n0.007767 0.709224\n0.007767 0.419730\n0.007782 0.953124\n0.007782 0.151293\n0.007797 0.792569\n0.007828 0.225849\n0.007828 0.454566\n0.007828 0.806500\n0.007843 0.856703\n0.007858 0.342824\n0.007858 0.310552\n0.007889 0.450675\n0.008103 0.269856\n0.008118 0.902205\n0.008133 0.783154\n0.008133 0.655772\n0.008133 0.315373\n0.008148 0.935210\n0.008164 0.320653\n0.008179 0.988373\n0.008194 0.255589\n0.008194 0.673365\n0.008194 0.532509\n0.008225 0.664134\n0.008225 0.173434\n0.008225 0.336446\n0.008240 0.475731\n0.008240 0.868177\n0.008240 0.090715\n0.008255 0.342260\n0.008270 0.406241\n0.008270 0.739528\n0.008286 0.321569\n0.008301 0.948562\n0.008301 0.368856\n0.008301 0.619226\n0.008301 0.941176\n0.008301 0.927596\n0.008316 0.176257\n0.008316 0.549462\n0.008316 0.781842\n0.008331 0.360296\n0.008347 0.380652\n0.008377 0.080125\n0.008377 0.923247\n0.008392 0.679133\n0.008392 0.561563\n0.008408 0.978714\n0.008408 0.997635\n0.008423 0.888685\n0.008438 0.849516\n0.008438 0.946563\n0.008453 0.351522\n0.008453 0.867124\n0.008469 0.994369\n0.008469 0.887770\n0.008469 0.750866\n0.008484 0.328878\n0.008515 0.476280\n0.008560 0.405829\n0.008576 0.492866\n0.008591 0.038361\n0.008606 0.066499\n0.008606 0.603830\n0.008606 0.615137\n0.008621 0.995300\n0.008637 0.448798\n0.008637 0.740154\n0.008637 0.295537\n0.008652 0.636484\n0.008652 0.734508\n0.008652 0.817319\n0.008667 0.014755\n0.008667 0.429099\n0.008682 0.493172\n0.008682 0.691173\n0.008682 0.590845\n0.008698 0.768338\n0.008713 0.112337\n0.008713 0.928527\n0.008728 0.980804\n0.008728 0.677623\n0.008728 0.882368\n0.008743 0.963195\n0.008743 0.942504\n0.008743 0.704173\n0.008743 0.886122\n0.008743 0.917494\n0.008759 0.669841\n0.008789 0.197467\n0.008789 0.408270\n0.008804 0.347692\n0.008820 0.974762\n0.008850 0.088212\n0.008865 0.239261\n0.008865 0.432685\n0.008881 0.376318\n0.008881 0.273335\n0.008896 0.225299\n0.008896 0.032822\n0.009094 0.356451\n0.009110 0.781735\n0.009110 0.111635\n0.009140 0.164736\n0.009140 0.605249\n0.009155 0.743618\n0.009155 0.614755\n0.009171 0.344671\n0.009171 0.961761\n0.009171 0.861250\n0.009186 0.589273\n0.009201 0.702571\n0.009201 0.283497\n0.009201 0.191119\n0.009201 0.263523\n0.009201 0.516732\n0.009232 0.242145\n0.009232 0.181903\n0.009232 0.568643\n0.009247 0.252201\n0.009247 0.170520\n0.009247 0.557641\n0.009262 0.369421\n0.009262 0.299382\n0.009277 0.615854\n0.009277 0.676326\n0.009293 0.340627\n0.009308 0.386587\n0.009308 0.789883\n0.009308 0.056870\n0.009323 0.825834\n0.009354 0.390860\n0.009354 0.309880\n0.009354 0.339406\n0.009354 0.024842\n0.009354 0.974151\n0.009369 0.199207\n0.009384 0.757168\n0.009384 0.654643\n0.009384 0.533150\n0.009400 0.422889\n0.009400 0.093431\n0.009400 0.149630\n0.009400 0.712169\n0.009415 0.421225\n0.009430 0.397726\n0.009491 0.453666\n0.009506 0.027893\n0.009522 0.126131\n0.009537 0.474479\n0.009552 0.072343\n0.009552 0.282414\n0.009567 0.999741\n0.009567 0.708751\n0.009583 0.418845\n0.009598 0.316747\n0.009598 0.983719\n0.009598 0.818143\n0.009613 0.456107\n0.009613 0.090867\n0.009628 0.398154\n0.009659 0.295674\n0.009659 0.016205\n0.009659 0.628687\n0.009674 0.292683\n0.009674 0.798611\n0.009674 0.759426\n0.009689 0.966217\n0.009705 0.927840\n0.009720 0.492699\n0.009735 0.185107\n0.009735 0.894270\n0.009751 0.852888\n0.009751 0.274891\n0.009781 0.868528\n0.009781 0.671870\n0.009812 0.600336\n0.009812 0.748257\n0.009812 0.587549\n0.009827 0.692119\n0.009842 0.532296\n0.009842 0.138354\n0.009857 0.173465\n0.009857 0.460197\n0.009857 0.267887\n0.009857 0.096178\n0.009873 0.036255\n0.009873 0.837827\n0.009888 0.034165\n0.009888 0.943328\n0.010117 0.663172\n0.010132 0.737133\n0.010147 0.251362\n0.010147 0.221607\n0.010147 0.793759\n0.010147 0.445151\n0.010163 0.156619\n0.010163 0.217166\n0.010163 0.687297\n0.010178 0.468635\n0.010193 0.071519\n0.010193 0.433509\n0.010224 0.266712\n0.010224 0.296620\n0.010224 0.138399\n0.010239 0.005432\n0.010239 0.880430\n0.010239 0.511742\n0.010254 0.147295\n0.010254 0.622721\n0.010269 0.046784\n0.010269 0.181796\n0.010269 0.428656\n0.010269 0.960815\n0.010269 0.973281\n0.010285 0.313832\n0.010285 0.953414\n0.010285 0.112139\n0.010300 0.169467\n0.010315 0.330114\n0.010315 0.111635\n0.010315 0.140780\n0.010315 0.482826\n0.010315 0.588739\n0.010346 0.535744\n0.010346 0.677668\n0.010361 0.842847\n0.010376 0.205417\n0.010376 0.629389\n0.010376 0.382681\n0.010391 0.597223\n0.010391 0.829816\n0.010391 0.956863\n0.010391 0.889784\n0.010391 0.327733\n0.010422 0.123552\n0.010422 0.300877\n0.010437 0.847608\n0.010437 0.392264\n0.010437 0.508293\n0.010437 0.617884\n0.010437 0.136156\n0.010437 0.404822\n0.010452 0.016541\n0.010468 0.231525\n0.010468 0.579400\n0.010483 0.196201\n0.010483 0.701244\n0.010498 0.865797\n0.010498 0.812665\n0.010498 0.340383\n0.010498 0.890578\n0.010498 0.519203\n0.010513 0.175113\n0.010513 0.014542\n0.010529 0.337636\n0.010529 0.837110\n0.010544 0.636362\n0.010544 0.063798\n0.010559 0.245502\n0.010559 0.054414\n0.010559 0.568200\n0.010575 0.603799\n0.010575 0.301244\n0.010590 0.447639\n0.010590 0.625452\n0.010590 0.331823\n0.010605 0.418677\n0.010605 0.277440\n0.010605 0.263279\n0.010620 0.862287\n0.010681 0.191562\n0.010712 0.966247\n0.010742 0.713207\n0.010742 0.049104\n0.010742 0.684489\n0.010788 0.605310\n0.010803 0.307591\n0.010819 0.026535\n0.010819 0.909285\n0.010819 0.727306\n0.010819 0.234348\n"
     ]
    }
   ],
   "source": [
    "for i1,i2 in sx[:1000]:\n",
    "    print(\"{:.6f} {:.6f}\".format(i1,i2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.ensemble import GradientBoostingRegressor\n",
    " from sklearn.ensemble import RandomForestRegressor\n",
    " from sklearn.linear_model import LinearRegression\n",
    " from sklearn.ensemble import VotingRegressor\n",
    " reg1 = GradientBoostingRegressor(random_state=1, n_estimators=50)\n",
    " reg2 = RandomForestRegressor(random_state=1, n_estimators=50)\n",
    " reg3 = LinearRegression()\n",
    " ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = getpolyxandy(f+'/train.csv',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fit once\n",
      "loss for f is  0.9068700797783636\n",
      "fit once\n",
      "loss for f is  0.9106253931652961\n",
      "fit once\n",
      "loss for f is  0.9374998859191009\n",
      "fit once\n",
      "loss for f is  0.7311570957621324\n",
      "fit once\n",
      "loss for f is  0.7410219112178085\n",
      "fit once\n",
      "loss for f is  0.805972725642053\n",
      "fit once\n",
      "loss for f is  0.6739295682130005\n",
      "fit once\n",
      "loss for f is  0.7058138351283785\n",
      "fit once\n",
      "loss for f is  2.8534801530309846\n",
      "final average is  1.029596738650791\n"
     ]
    }
   ],
   "source": [
    "finalacc = []\n",
    "for f in folder[9:]:\n",
    "  X,y = getxandy(f+'/train.csv')\n",
    "  reg = ereg\n",
    "  for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    reg.fit(X_train,y_train)\n",
    "    print(\"fit once\")\n",
    "    break\n",
    "  y_pred = reg.predict(X_test)\n",
    "  loss = scoreperdataset(y_pred, y_test) \n",
    "  finalacc.append(loss)\n",
    "  print('loss for f is ', loss)\n",
    "print('final average is ', sum(finalacc)/len(finalacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}